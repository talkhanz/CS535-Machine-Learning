# -*- coding: utf-8 -*-
"""CS535_21100313_A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cm00opUytJmJyK8Kwm9Iwik-LOOdmHoK

# Programming Assignment 2: Linear Regression

## Instructions

- The aim of this assignment is to give you a hands-on with a real-life machine learning application.
- Use separate training, and testing data as discussed in class.
- You can only use Python programming language and Jupyter Notebooks.
- There are three parts of this assignment. In parts 1 & 2, you can only use **numpy, scipy, pandas, matplotlib and are not allowed to use NLTK, scikit-learn or any other machine learning toolkit**. However, you have to use **scikit-learn** in part 3.
- Carefully read the submission instructions, plagiarism and late days policy below.
- Deadline to submit this assignment is: **Friday, 30th October 2020**.

## Submission Instructions

Submit your code both as notebook file (.ipynb) and python script (.py) on LMS. The name of both files should be your roll number. If you don’t know how to save .ipynb as .py [see this](https://i.stack.imgur.com/L1rQH.png). **Failing to submit any one of them will result in the reduction of marks**.

## Plagiarism Policy

The code MUST be done independently. Any plagiarism or cheating of work from others or the internet will be immediately referred to the DC. If you are confused about what constitutes plagiarism, it is YOUR responsibility to consult with the instructor or the TA in a timely manner. No “after the fact” negotiations will be possible. The only way to guarantee that you do not lose marks is “DO NOT LOOK AT ANYONE ELSE'S CODE NOR DISCUSS IT WITH THEM”.

## Late Days Policy

The deadline of the assignment is final. However, in order to accommodate all the 11th hour issues there is a late submission policy i.e. you can submit your assignment within 3 days after the deadline with 25% deduction each day.


## Introduction

In this exercise, you will implement linear regression and get to see it work on data. After completing this assignment, you will know:
- How to implement linear regression from scratch.
- How to estimate linear regression parameters using gradient descent.
- How to make predictions on new data using learned parameters.

Let's start with the necessary imports.
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
from matplotlib import pyplot
# %matplotlib inline

"""## 1. Linear Regression with one variable

Linear regression assumes a linear relationship between the input variables (X) and the single output variable (Y). More specifically, that output (Y) can be calculated from a linear combination of the input variables (X). When there is a single input variable, the method is referred to as a simple linear regression.

Now you will implement simple linear regression to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next.

### 1.1 Load the dataset

The file `Data/ex1data1.txt` contains the dataset for our linear regression problem. The first column is the population of a city (in 10,000s) and the second column is the profit of a food truck in that city (in $10,000s). A negative value for profit indicates a loss. 

We provide you with the code needed to load this data. The dataset is loaded from the data file into the variables `X` and `Y`.
"""

data = np.loadtxt(os.path.join('Data', 'ex1data.txt'), delimiter=',')
X, Y = data[:, 0], data[:, 1]

"""### 1.2 Plot the dataset
Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). Execute the next cell to visualize the data.
"""

pyplot.plot(X, Y, 'ro', ms=10, mec='k')
pyplot.ylabel('Profit in $10,000')
pyplot.xlabel('Population of City in 10,000s')

"""### 1.3 Learn the parameters
In this part, you will fit the linear regression parameters $\theta$ to the food truck dataset using gradient descent.

The objective of linear regression is to minimize the cost function

$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)^2 ------ (i)$$ 

where the hypothesis $h_\theta(x)$ is given by the linear model
$$ h_\theta(x) = \theta_0 + \theta_1 x ------ (ii)$$

The parameters of your model are the $\theta_j$ values. These are
the values you will adjust to minimize cost $J(\theta)$. One way to do this is to
use the batch gradient descent algorithm. In batch gradient descent, each
iteration performs the update

$$ \theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right) ------ (iii)$$

$$ \theta_1 = \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)x^{(i)} ------ (iv)$$

With each step of gradient descent, your parameters $\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\theta$).

Let's start by implementing the hypothesis $h_\theta(x)$.
"""

### GRADED FUNCTION ###
def predict(x, theta0, theta1):
    '''
    Calculates the hypothesis for any input sample `x` given the parameters `theta`.
    
    Arguments
    ---------
    x : float
        The input sample.
    
    theta0 : float
        The parameter for the regression function.
        
    theta1 : float
        The parameter for the regression function.
    
    Returns
    -------
    h_x : float
        The hypothesis for input sample.
    
    Hint(s)
    -------
    Compute equation (ii).
    '''
    # You need to return the following variable(s) correctly
    h_x = 0.0
    
    ### START CODE HERE ### (≈ 1 line of code)
    h_x = theta0 + x * theta1
    
    ### END CODE HERE ###
    
    return h_x

"""Execute the next cell to verify your implementation."""

h_x = predict(x=2, theta0=1.0, theta1=1.0)
print('With x = 2, theta0 = 1.0, theta1 = 1.0\nPredicted Hypothesis h(x) = %.2f' % h_x)
print("Expected hypothesis h(x) = 3.00\n")

"""As you perform gradient descent to learn minimize the cost function  $J(\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate  $J(\theta)$ so you can check the convergence of your gradient descent implementation."""

### GRADED FUNCTION ###
def computeCost(X, Y, theta0, theta1):
    '''
    Computes cost for linear regression. Computes the cost of using `theta` as the
    parameter for linear regression to fit the data points in `X` and `Y`.
    
    Arguments
    ---------
    X : array
        The input dataset of shape (m, ), where m is the number of training examples.
    
    Y : array
        The values of the function at each data point. This is a vector of
        shape (m, ), where m is the number of training examples.
    
    theta0 : float
        The parameter for the regression function.
        
    theta1 : float
        The parameter for the regression function.
    
    Returns
    -------
    J : float
        The value of the regression cost function.
    
    Hint(s)
    -------
    Compute equation (i).
    '''
    # initialize some useful values
    m = Y.size  # number of training examples
    
    # You need to return the following variable(s) correctly
    J = 0
        
    ### START CODE HERE ### (≈ 3-4 lines of code)
    total_error = 0 
    for i in range(m):
      h_x_i = predict(x=X[i],theta0=theta0,theta1=theta1)
      diff_squared = (h_x_i - Y[i]) ** 2
      total_error  = total_error + diff_squared
    J = (1/(2*m)) * total_error
    ### END CODE HERE ###
    return J

"""Execute the next cell to verify your implementation."""

J = computeCost(X, Y, theta0=1.0, theta1=1.0)
print('With theta0 = 1.0, theta1 = 1.0\nPredicted cost J = %.2f' % J)
print("Expected cost J = 10.27\n")

"""Next, you will complete a function which implements gradient descent. The loop structure has been written for you, and you only need to supply the updates to parameters $\theta_j$  within each iteration (epoch). 

The starter code for the function `gradientDescent` calls `computeCost` on every iteration and saves the cost to a `python` list. Assuming you have implemented `gradientDescent` and `computeCost` correctly, your value of $J(\theta)$ should never increase, and should converge to a steady value by the end of the algorithm.
"""

### GRADED FUNCTION ###
def gradientDescent(X, Y, alpha, n_epoch):
    """
    Performs gradient descent to learn `theta`. Updates `theta` by taking `n_epoch`
    gradient steps with learning rate `alpha`.
    
    Arguments
    ---------
    X : array
        The input dataset of shape (m, ), where m is the number of training examples.
    
    Y : array
        The values of the function at each data point. This is a vector of
        shape (m, ), where m is the number of training examples.
    
    alpha : float
        The learning rate.
    
    n_epoch : int
        The number of iterations for gradient descent. 
    
    Returns
    -------
    theta0 : float
        The parameter for the regression function.
        
    theta1 : float
        The parameter for the regression function.
    
    J : list
        A python list for the values of the cost function after each iteration.
    
    Hint(s)
    -------
    Compute equation (iii) and (iv).

    While debugging, it can be useful to print out the values of 
    the cost function (computeCost) here.
    """
    # initialize some useful values
    m = Y.size  # number of training examples
    J = list()  # list to store cost
    
    # You need to return the following variables correctly
    theta0 = 0.0
    theta1 = 0.0
    
    for epoch in range(n_epoch):
        ### START CODE HERE ### (≈ 5-10 lines of code)
        error_theta0 = 0
        error_theta1 = 0 
        for i in range(m):
          error_theta0 = error_theta0+  ( predict(X[i],theta0,theta1) - Y[i] ) 
          error_theta1 = error_theta1+  ( predict(X[i],theta0,theta1) - Y[i] )*X[i]
        deriv_theta0 = (1/m) * error_theta0
        deriv_theta1 = (1/m) * error_theta1
        theta0 = theta0 - alpha * deriv_theta0
        theta1 = theta1 - alpha * deriv_theta1
        ### END CODE HERE ###

        J.append(computeCost(X, Y, theta0, theta1))
    return theta0, theta1, J

"""Execute the next cell to verify your implementation."""

n_epoch = 1500
alpha = 0.01

theta0, theta1, J = gradientDescent(X ,Y, alpha, n_epoch)
print('Predicted theta0 = %.4f, theta1 = %.4f, cost = %.4f' % (theta0, theta1, J[-1]))
print('Expected theta0 = -3.6303, theta1 = 1.1664, cost = 4.4834')

"""### 1.4 Plot the linear fit

Use your learned parameters $\theta_j$ to plot the linear fit.
"""

h_x = list()
for x in X:
    h_x.append(predict(x, theta0, theta1))
pyplot.plot(X, Y, 'ro', ms=10, mec='k')
pyplot.ylabel('Profit in $10,000')
pyplot.xlabel('Population of City in 10,000s')
pyplot.plot(X, h_x, '-')
pyplot.legend(['Training data', 'Linear regression'])

"""### 1.5 Make predictions

Use your learned parameters $\theta_j$ to make food truck profit predictions in areas with population of 40,000 and 65,000.
"""

print('For population = 40,000, predicted profit = $%.2f' % (predict(4, theta0, theta1)*10000))
print('For population = 65,000, predicted profit = $%.2f' % (predict(6.5, theta0, theta1)*10000))

"""## 2. Multivariate Linear Regression

Now, you will implement multivariate linear regression (from scratch) to predict the the median price of homes in a Boston suburb during the mid-1970s. To do this, you are given with the dataset that has 404 examples in the train set and 102 examples in test set. Each example has 13 input variables (features) and one output variable (price in $10,000s). Below is the description of input variables:

- Per capita crime rate.
- The proportion of residential land zoned for lots over 25,000 square feet.
- The proportion of non-retail business acres per town.
- Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- Nitric oxides concentration (parts per 10 million).
- The average number of rooms per dwelling.
- The proportion of owner-occupied units built before 1940.
- Weighted distances to five Boston employment centers.
- Index of accessibility to radial highways.
- Full-value property-tax rate per $10,000.
- Pupil-teacher ratio by town.
- 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.
- Percentage lower status of the population.

Each one of these input features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. This is often the case with real-world data, and understanding how to explore and clean such data is an important skill to develop.

A common way to normalize features that use different scales and ranges is:

- Subtract the mean value of each feature from the dataset.
- After subtracting the mean, additionally scale (divide) the feature values by their respective standard deviations.

Note: We only use examples of the train set to estimate the mean and standard deviation.

You have to follow exactly the same steps as above i.e. implement hypothesis, cost function and gradient descent for multivariate linear regression to learn parameters $\theta$ using train set. Finally, report the cost (error) using your learned parameters $\theta$ on test set. Expected Mean Square Error on this dataset is 11.5 - 12.5 approximately. 

We provide you with the code needed to load this dataset. The dataset is loaded from the data files into the variables `train_X`, `train_Y`, `test_X` and `test_Y`.
"""

train_Y = np.loadtxt(os.path.join('Data', 'ex2trainlabels.txt'))
test_Y = np.loadtxt(os.path.join('Data', 'ex2testlabels.txt'))

''' Start your code of part 2 from here, add the new code cells as per your requirement. '''
def append_one(X):
  new_X = []
  for x in X:
    new_x = np.insert(x,0,1,axis=0)
    new_X.append(new_x)
  new_X = np.array(new_X)
  return new_X
def predict_multi_v1(X,thetas):
  '''
  x --one training instance 
  thetas -- a list containing all thetas
  '''
  h_x = np.dot(X.transpose(),thetas)
  return h_x

def computeCost_multi_v1(X,Y,thetas):
    # initialize some useful values
    m = Y.size  # number of training examples
    # You need to return the following variable(s) correctly
    J = 0        
    ### START CODE HERE ### (≈ 3-4 lines of code)
    total_error = 0 
    h_x = predict_multi_v1(X,thetas)
    diff = np.subtract(h_x,Y)
    diff_squared = np.power(diff,2)
    total_error  = np.sum(diff_squared)
    J = (1/(2*m)) * total_error
    ### END CODE HERE ###
    return J

def gradientDescent_multi_v1(X, Y, alpha, n_epoch):
  m = Y.size  # number of training examples
  J = list()  # list to store cost
  thetas = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0])
  h_x = predict_multi_v1(X,thetas)
  deriv_theta = np.zeros(thetas.shape)
  for epoch in range(n_epoch):
    h_x = predict_multi_v1(X,thetas)
    diff = np.subtract(h_x,Y)
    for j in range(14):
      error = []
      for i in range(404):
        feature_j = X[j][i]
        error.append(diff[i] * feature_j)
      error = np.array(error)
      sum_error = np.sum(error)
      deriv_theta[j] = (1/m) * sum_error
    thetas = thetas - deriv_theta*alpha
    J.append(computeCost_multi_v1(X,Y,thetas))
  return J,thetas
def extract_each_feature_v1(X):
  features = []
  len_X = len(X[0])
  for i in range(len_X):
    feature = X[:,i]
    features.append(feature)
  return features
def get_training_feature_stats_v1(features):
  means = []
  stds = []
  for feature in features:
    m = np.mean(feature)
    s = np.std(feature)
    means.append(m)
    stds.append(s)
  return means,stds

def feature_scaling_test_v1(features,train_means,train_stds):
  scaled_features = []
  i = 0
  for feature in features:
    m = train_means[i]
    s = train_stds[i]
    if s ==0 :
      s = 1
    new_f = np.array([(f - m)/s for f in feature])

    scaled_features.append(new_f)
    i = i + 1
  scaled_features = np.array(scaled_features)
  return scaled_features
def feature_scaling_train_v1(features):
  scaled_features = []
  for feature in features:
    m = np.mean(feature)
    s = np.std(feature)
    if s == 0:
      s = 1
    new_f = np.array([(f - m)/s for f in feature])
    scaled_features.append(new_f)
  scaled_features = np.array(scaled_features)
  return scaled_features

train_X = np.loadtxt(os.path.join('Data', 'ex2traindata.txt'))
train_f = extract_each_feature_v1(train_X)
train_f_scaled = feature_scaling_train_v1(train_f)
train_f_scaled = append_one(train_f_scaled.transpose())
train_f_scaled= train_f_scaled.transpose()
alpha = 0.01
epochs = 1500
import datetime
start_time = datetime.datetime.now()
J,thetas = gradientDescent_multi_v1(train_f_scaled,train_Y,alpha,epochs)
finish_time = datetime.datetime.now() - start_time

print('gradient descent finished in ',finish_time)

print("best value of thetas are",thetas)

def compute_mean_square_error(predicted,actual):
  error = 0 
  n =len(predictions)
  for i in range(n):
    d = (predicted[i] - actual[i])**2
    error = error + d
  mse = error / (2* n)
  return mse

test_X = np.loadtxt(os.path.join('Data', 'ex2testdata.txt'))
test_X = np.array(test_X)
test_f = extract_each_feature_v1(test_X)
train_means,train_stds = get_training_feature_stats_v1(train_f)
test_f_scaled = feature_scaling_test_v1(test_f,train_means,train_stds)
test_f_scaled = append_one(test_f_scaled.transpose())
test_f_scaled= test_f_scaled.transpose()

predictions = predict_multi_v1(test_f_scaled,thetas)
mse = compute_mean_square_error(predictions,test_Y)
print("mse is ",mse)

"""## 3. Regularized Linear Regression

Now, you'll use the [scikit-learn](https://scikit-learn.org/stable/index.html) to implement [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) and apply them to Boston house pricing dataset (provided in part 2). Try out different values of regularization coefficient (known as alpha in scikit-learn) and use the [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) to report loss with each regression. Finally, plot the regularization coefficients alpha (x-axis) with learned parameters $\theta$ (y-axis) for Ridge and Lasso. Please read [this blog](https://scienceloft.com/technical/understanding-lasso-and-ridge-regression/) to get better understanding of the desired plots.
"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

''' Start your code of part 3 from here, add the new code cells as per your requirement. '''
test_X = np.loadtxt(os.path.join('Data', 'ex2testdata.txt'))
train_X = np.loadtxt(os.path.join('Data', 'ex2traindata.txt'))
test_X = append_one(test_X)
train_X = append_one(train_X)
lr = LinearRegression(normalize= True).fit(train_X, train_Y)

lr_pred = lr.predict(test_X)
print('mse for linear regression is',mean_squared_error(test_Y,lr_pred))

alphas =np.logspace(-2,5,10)
alphas = np.insert(alphas,0,0) # just to show my mse at 0 is around 23. you can comment this line to remove warnings
coefs = []
mses = []
for al in alphas:
  ridge = Ridge(alpha = al,normalize= True)
  ridge.fit(train_X,train_Y)
  Y_pred = ridge.predict(test_X)
  mse = mean_squared_error(Y_pred,test_Y)
  mses.append(mse)
  coefs.append(ridge.coef_)
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('theta weights')
plt.title('Ridge coefficients')
plt.show()

print('mse at alpha=0 is',mses[0])
ax = plt.gca()
plt.plot(alphas,mses)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('Ridge MSE')
plt.title('MSE vs Alpha')
plt.show()

alphas =np.logspace(-2,5,10)
alphas = np.insert(alphas,0,0) # just to show my mse at 0 is around 23. you can comment this line to remove warnings
coefs = []
mses = []
for al in alphas:
  lasso = Lasso(alpha = al,normalize= True)
  lasso.fit(train_X,train_Y)
  Y_pred = lasso.predict(test_X)
  mse = mean_squared_error(Y_pred,test_Y)
  mses.append(mse)
  coefs.append(lasso.coef_)
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('theta weights')
plt.title('Lasso coefficients')
plt.show()

print('mse at alpha=0 is',mses[0])
ax = plt.gca()
plt.plot(alphas,mses)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('Lasso MSE')
plt.title('MSE vs Alpha')
plt.show()

alphas =np.logspace(-2,5,10)
alphas = np.insert(alphas,0,0) # just to show my mse at 0 is around 23. you can comment this line to remove warnings
coefs = []
mses = []
for al in alphas:
  el = ElasticNet(alpha = al,normalize= True)
  el.fit(train_X,train_Y)
  Y_pred = el.predict(test_X)
  mse = mean_squared_error(Y_pred,test_Y)
  mses.append(mse)
  coefs.append(el.coef_)
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('theta weights')
plt.title('Elastic Net coefficients')
plt.show()

print('mse at alpha=0 is',mses[0])
ax = plt.gca()
plt.plot(alphas,mses)
ax.set_xscale('log')
plt.xlabel('log alpha')
plt.ylabel('Enet MSE')
plt.title('MSE vs Alpha')
plt.show()