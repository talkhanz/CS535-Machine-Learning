{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T26WbTkPdQjv",
    "outputId": "b20666a8-e7ab-4dcc-b88e-f0d134b1991d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing part 1 -----------------------------------------\n",
      "starting training phase\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_neg_reviews.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_pos_reviews.data\n",
      "generating training vocab\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/training_vocab.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_pos_reviews_words.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_neg_reviews_words.data\n",
      "train_vocab generated of 74739 words\n",
      "starting training of naive_bayes_classifier\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/training_vocab.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_pos_reviews_words.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_neg_reviews_words.data\n",
      "completed training of naive_bayes_classifier\n",
      "finished traininig phase\n",
      "starting testing phase\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/test_neg_reviews.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/test_pos_reviews.data\n",
      "the accuracy is: 0.82564\n",
      "finished testing phase\n",
      "executed part 1 in 0:09:27.780060 -----------------------------------------\n",
      "executing part 2 -----------------------------------------\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_neg_reviews.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/train_pos_reviews.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/test_neg_reviews.data\n",
      "pickled data loaded as pickle dump from F:\\LUMS\\F20\\ML\\Assignments\\submissions\\a4/test_pos_reviews.data\n",
      "testing acc is 0.82392\n",
      "confusion matrix is\n",
      " [[11017  2919]\n",
      " [ 1483  9581]]\n",
      "executed part 2 in 0:00:14.007041 -----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CS535_21100313_A4.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1BcCDka4rhXzIf-TKex4pvYCvEzxIxjD8\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "#change  datasetpath to where dataset is stored or create a Dataset folder in directory you run this file\n",
    "dataset_path  = os.getcwd() + \"/Dataset\"\n",
    "train_logs_path = os.getcwd()+ \"/train_logs.data\"\n",
    "train_raw_pos_reviews_path = os.getcwd() + \"/train_raw_pos_reviews.data\"\n",
    "train_raw_neg_reviews_path = os.getcwd() + \"/train_raw_neg_reviews.data\"\n",
    "train_pos_reviews_path = os.getcwd() + \"/train_pos_reviews.data\"\n",
    "train_neg_reviews_path = os.getcwd() + \"/train_neg_reviews.data\"\n",
    "test_pos_reviews_path = os.getcwd() + \"/test_pos_reviews.data\"\n",
    "test_neg_reviews_path = os.getcwd() + \"/test_neg_reviews.data\"\n",
    "train_bigdoc_path = os.getcwd() + \"/train_bigdoc.data\"\n",
    "test_bigdoc_path = os.getcwd() + \"/test_bigdoc.data\"\n",
    "train_pos_reviews_words_path = os.getcwd() + \"/train_pos_reviews_words.data\"\n",
    "train_neg_reviews_words_path = os.getcwd() + \"/train_neg_reviews_words.data\"\n",
    "test_pos_reviews_words_path = os.getcwd() + \"/test_pos_reviews_words.data\"\n",
    "test_neg_reviews_words_path = os.getcwd() + \"/test_neg_reviews_words.data\"\n",
    "training_vocab_path = os.getcwd() + \"/training_vocab.data\"\n",
    "testing_vocab_path = os.getcwd() + \"/testing_vocab.data\"\n",
    "training_vocab_dict_path = os.getcwd() + \"/training_vocab_dict.data\"\n",
    "training_vocab_index_dict_path = os.getcwd() + \"/training_vocab_index_dict.data\"\n",
    "training_document_path = os.getcwd() + \"/training_document.data\"\n",
    "index_dict_path = os.getcwd() + \"/index_dict.data\"\n",
    "bow_path = os.getcwd() + \"/bow.data\"\n",
    "stopwords_path = dataset_path + \"/stop_words.txt\"\n",
    "pos_words_path = dataset_path + \"/positive_words.txt\"\n",
    "neg_words_path = dataset_path + \"/negative_words.txt\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "pos_train_path = train_path + \"/pos\"\n",
    "neg_train_path = train_path + \"/neg\"\n",
    "test_path = dataset_path + \"/test\"\n",
    "pos_test_path = test_path + \"/pos\"\n",
    "neg_test_path = test_path + \"/neg\"\n",
    "\n",
    "def read_file(filepath):\n",
    "    if '.txt' in filepath:\n",
    "        with open(filepath,'r',encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "    return file_content\n",
    "def read_words(filepath):\n",
    "    if '.txt' in filepath:\n",
    "        with open(filepath,'r') as f:\n",
    "            file_content = f.readlines()\n",
    "    return file_content\n",
    "def load_words(word_filepath):\n",
    "    word_list = read_words(word_filepath)\n",
    "    words = []\n",
    "    for line in word_list:\n",
    "        words.append(line.strip())\n",
    "    return words\n",
    "def read_reviews(path):\n",
    "    new_path = path + \"/*.*\"\n",
    "    filenames = glob.glob(new_path)\n",
    "    reviews = []\n",
    "    i = 0 \n",
    "    for filename in filenames:\n",
    "        review  = read_file(filename)\n",
    "        reviews.append(review)\n",
    "        i = i + 1\n",
    "    return reviews\n",
    "def remove_stopwords_from_reviews_words(reviews_words,stopwords):\n",
    "  stopless_review_words = []\n",
    "  stopwords_dict = dict.fromkeys(stopwords)\n",
    "  for word in reviews_words:\n",
    "    index = stopwords_dict.get(word,-1)\n",
    "    if index == -1:\n",
    "      stopless_review_words.append(word)\n",
    "  return stopless_review_words\n",
    "def remove_stopwords_from_reviews(reviews,stopwords):\n",
    "    stopless_review_list = []\n",
    "    for review in reviews:\n",
    "        split_review = [word.lower() for word in re.split(\"\\W+\",review) if word.lower() not in stopwords]\n",
    "        empty_char = ' '\n",
    "        stopless_review = empty_char.join(split_review)\n",
    "        stopless_review_list.append(stopless_review)\n",
    "    return stopless_review_list\n",
    "def preprocess_reviews(pos_path,neg_path):\n",
    "    pos_reviews = read_reviews(pos_path)\n",
    "    neg_reviews = read_reviews(neg_path)\n",
    "    stopwords = load_words(stopwords_path)\n",
    "    stopless_pos_reviews = remove_stopwords_from_reviews(pos_reviews,stopwords)\n",
    "    stopless_neg_reviews = remove_stopwords_from_reviews(neg_reviews,stopwords)\n",
    "    return [stopless_neg_reviews,stopless_pos_reviews]\n",
    "def load_test_reviews():\n",
    "    import datetime\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"starting preprocessing of test reviews\")\n",
    "    processed_reviews = preprocess_reviews(pos_test_path,neg_test_path)\n",
    "    finish_time = datetime.datetime.now() - start_time\n",
    "\n",
    "    print(\"finished preprocessing of test reviews in\",finish_time)\n",
    "    neg_reviews = processed_reviews[0]\n",
    "    pos_reviews = processed_reviews[1]\n",
    "    return [neg_reviews,pos_reviews]\n",
    "def load_train_reviews():\n",
    "    import datetime\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"starting preprocessing of train reviews\")\n",
    "    processed_reviews = preprocess_reviews(pos_train_path,neg_train_path)\n",
    "    finish_time = datetime.datetime.now() - start_time\n",
    "\n",
    "    print(\"finished preprocessing of train reviews in\",finish_time)\n",
    "    neg_reviews = processed_reviews[0]\n",
    "    pos_reviews = processed_reviews[1]\n",
    "    return [neg_reviews,pos_reviews]\n",
    "#creates a long string of all reviews\n",
    "def create_corpus_from_reviews(reviews):\n",
    "    corpus = ''\n",
    "    for review in reviews:\n",
    "        corpus = corpus + ' '+review.lower() + ' '\n",
    "    return corpus\n",
    "def remove_empty_words_from_vocab(unique_words):\n",
    "    new_unique_words = []\n",
    "    for word in unique_words:\n",
    "        if len(word)!= 0 and word != ' ' :\n",
    "            new_unique_words.append(word)\n",
    "    return new_unique_words\n",
    "#this function takes train reviews and computes all the unique words in train reviews \n",
    "def create_vocabulary(neg_reviews,pos_reviews):\n",
    "    neg_corpus = create_corpus_from_reviews(neg_reviews)\n",
    "    pos_corpus = create_corpus_from_reviews(pos_reviews)\n",
    "    corpus = neg_corpus + pos_corpus\n",
    "    unique_words = list (set (re.split(\"\\W+\",corpus)))\n",
    "    return unique_words \n",
    "def create_training_vocabulary_from_documents(documents):\n",
    "    corpus = create_corpus_from_reviews(document)\n",
    "    unique_words = list (set (re.split(\"\\W+\",corpus)))\n",
    "    return unique_words\n",
    "def convert_vocab_into_dict(unique_words):\n",
    "    unique_words_dict = {}\n",
    "    for word in unique_words:\n",
    "        unique_words_dict[word] = 0\n",
    "    return unique_words_dict\n",
    "def word2index(word,unique_words_dict):\n",
    "    index = 0 \n",
    "    for key in unique_words_dict:\n",
    "        if key == word:\n",
    "            break\n",
    "        index = index + 1\n",
    "    if index == len(unique_words_dict):\n",
    "        index = -1\n",
    "    return index\n",
    "def save_pickled_data(data,path):\n",
    "    with open(path,'wb' ,) as f:\n",
    "        pickle.dump(data,f)\n",
    "    print(\"pickled data saved as pickle dump at\",path)\n",
    "def load_pickled_data(path):\n",
    "    with open(path,'rb' ,) as f:\n",
    "        data = pickle.load(f)\n",
    "    print(\"pickled data loaded as pickle dump from\",path)\n",
    "    return data\n",
    "def split_review_into_list_of_words(review):\n",
    "    words = review.split()\n",
    "    return words\n",
    "\n",
    "#conveerts [['hey],'['bye']] to ['hey','bye']   \n",
    "def create_words_from_reviews(reviews):\n",
    "  reviews_words =  []\n",
    "  for i in range(len(reviews)):\n",
    "    words = split_review_into_list_of_words(reviews[i])\n",
    "    for word in words:\n",
    "      reviews_words.append(word)\n",
    "  return reviews_words\n",
    "\n",
    "def count(w,big_doc,c):\n",
    "  \n",
    "  count = big_doc[c][w]\n",
    "  return count\n",
    "  \n",
    "\n",
    "def train_naive_bayes_classifier(D,C):\n",
    "  from collections import Counter\n",
    "  V = load_pickled_data(training_vocab_path)\n",
    "  train_pos_reviews_words = load_pickled_data(train_pos_reviews_words_path)\n",
    "  train_neg_reviews_words = load_pickled_data(train_neg_reviews_words_path)\n",
    "  nd = len(D)\n",
    "  nc = 12500\n",
    "  log_likelihood =[{},{}]\n",
    "  bigdoc = [[''],['']]\n",
    "  log_prior = [0,0]\n",
    "  for c in C:\n",
    "    log_prior[c]  = np.log(nc/nd)\n",
    "    if c == 0:\n",
    "      bigdoc[c] = Counter(train_neg_reviews_words)\n",
    "    else:\n",
    "      bigdoc[c] = Counter(train_pos_reviews_words)\n",
    "    for word in V:\n",
    "      count_w_c = count(word,bigdoc,c)\n",
    "      count_of_all_words = sum(bigdoc[c].values())\n",
    "      log_likelihood_w_c = np.log( (count_w_c + 1)/ (count_of_all_words + len(V)))\n",
    "      log_likelihood[c][word] = log_likelihood_w_c \n",
    "  return [log_prior,log_likelihood]\n",
    "\n",
    "def compute_accuracy(predictions,ground_truth):\n",
    "  total = len(predictions) \n",
    "  l = len(predictions)\n",
    "  match = 0\n",
    "  for i in range(l):\n",
    "    if predictions[i] == ground_truth[i]:\n",
    "      match = match + 1\n",
    "  accuracy = match / total\n",
    "  return accuracy\n",
    "\n",
    "def test_naive_bayes_classifier(test_doc,log_prior,log_likelihood,C,V):\n",
    "  sum = [0,0]\n",
    "  vocab_dict = convert_vocab_into_dict(V)\n",
    "  for c in C:\n",
    "    sum[c] = log_prior[c]\n",
    "    for i in range(len(test_doc)):\n",
    "      word = test_doc[i]\n",
    "      index = vocab_dict.get(word,-1)\n",
    "      if index != -1:\n",
    "        sum[c] = sum[c] + log_likelihood[c][word]\n",
    "  sum = np.array(sum)\n",
    "  return np.argmax(sum)\n",
    "\n",
    "\n",
    "def execute_part1():\n",
    "    files_pickled = True # set to False , if you want to run from scratch or true if you want cached version e.g variables/files like vocab/preprocessing data are already processed as pickle files so it runs faster\n",
    "    print(\"executing part 1 -----------------------------------------\")\n",
    "    import datetime\n",
    "    start_time = datetime.datetime.now()\n",
    "    if not files_pickled:\n",
    "        train_reviews = load_train_reviews()\n",
    "        train_neg_reviews = train_reviews[0] # processed data\n",
    "        train_pos_reviews = train_reviews[1]\n",
    "        train_pos_reviews_words =  create_words_from_reviews(train_pos_reviews)\n",
    "        train_neg_reviews_words =  create_words_from_reviews(train_neg_reviews)\n",
    "        save_pickled_data(train_pos_reviews_words,train_pos_reviews_words_path)\n",
    "        save_pickled_data(train_neg_reviews_words,train_neg_reviews_words_path)\n",
    "        documents = train_neg_reviews + train_pos_reviews\n",
    "        save_pickled_data(documents,training_document_path)\n",
    "        print(\"generating training vocab\")\n",
    "        training_vocab = create_vocabulary(train_neg_reviews,train_pos_reviews)\n",
    "        training_vocab = remove_empty_words_from_vocab(training_vocab)\n",
    "        print(\"train_vocab generated of\",len(training_vocab),\"words\")\n",
    "        save_pickled_data(training_vocab,training_vocab_path)\n",
    "        classes = [0,1]\n",
    "        print(\"starting training of naive_bayes_classifier\")\n",
    "        logs = train_naive_bayes_classifier(documents,classes)\n",
    "        log_prior = logs[0]\n",
    "        log_likelihood = logs[1]\n",
    "        print(\"completed training of naive_bayes_classifier\")\n",
    "        print(\"starting testing phase\")\n",
    "        test_reviews = load_test_reviews()\n",
    "        test_neg_reviews = test_reviews[0]\n",
    "        test_pos_reviews = test_reviews[1]\n",
    "        test_reviews = test_neg_reviews + test_pos_reviews\n",
    "        pred_labels = []\n",
    "        pos_labels =[ 1 for i in range(12500)]\n",
    "        neg_labels = [0 for i in range(12500)]\n",
    "        true_labels= neg_labels + pos_labels\n",
    "        C = [0,1]\n",
    "        for test_doc in test_reviews:\n",
    "          label = test_naive_bayes_classifier(test_doc.split(),log_prior,log_likelihodd,C,training_vocab)\n",
    "          pred_labels.append(label)\n",
    "        acc = compute_accuracy(pred_labels,true_labels)\n",
    "        print(\"the accuracy is:\", acc)\n",
    "        print(\"finished testing phase\")\n",
    "\n",
    "    else:\n",
    "        print(\"starting training phase\")\n",
    "        train_neg_reviews = load_pickled_data(train_neg_reviews_path) # processed data\n",
    "        train_pos_reviews = load_pickled_data(train_pos_reviews_path)\n",
    "        documents = train_neg_reviews + train_pos_reviews\n",
    "        classes = [0,1]\n",
    "        print(\"generating training vocab\")\n",
    "        training_vocab = load_pickled_data(training_vocab_path)\n",
    "        train_pos_reviews_words = load_pickled_data(train_pos_reviews_words_path) # each review is a list of words rather than string\n",
    "        train_neg_reviews_words = load_pickled_data(train_neg_reviews_words_path)\n",
    "        train_D = train_neg_reviews_words + train_pos_reviews_words\n",
    "        print(\"train_vocab generated of\",len(training_vocab),\"words\")\n",
    "        print(\"starting training of naive_bayes_classifier\")\n",
    "        logs = train_naive_bayes_classifier(train_D,classes)\n",
    "        log_prior = logs[0]\n",
    "        log_likelihood = logs[1]\n",
    "        print(\"completed training of naive_bayes_classifier\")\n",
    "        print(\"finished traininig phase\")\n",
    "        print(\"starting testing phase\")\n",
    "        test_neg_reviews = load_pickled_data(test_neg_reviews_path)\n",
    "        test_pos_reviews = load_pickled_data(test_pos_reviews_path)\n",
    "        test_reviews = test_neg_reviews + test_pos_reviews\n",
    "        pred_labels = []\n",
    "        pos_labels =[ 1 for i in range(12500)]\n",
    "        neg_labels = [0 for i in range(12500)]\n",
    "        true_labels= neg_labels + pos_labels\n",
    "        C = [0,1]\n",
    "        for test_doc in test_reviews:\n",
    "          label = test_naive_bayes_classifier(test_doc.split(),log_prior,log_likelihood,C,training_vocab)\n",
    "          pred_labels.append(label)\n",
    "        acc = compute_accuracy(pred_labels,true_labels)\n",
    "        print(\"the accuracy is:\", acc)\n",
    "        print(\"finished testing phase\")\n",
    "    finish_time = datetime.datetime.now() - start_time\n",
    "    print(\"executed part 1 in\",finish_time ,\"-----------------------------------------\") # just so you know how much time you should expect my program to run\n",
    "\n",
    "execute_part1()\n",
    "\n",
    "def execute_part2():\n",
    "    print(\"executing part 2 -----------------------------------------\")\n",
    "    import datetime\n",
    "    start_time = datetime.datetime.now()\n",
    "    train_neg_reviews = load_pickled_data(train_neg_reviews_path) # processed data\n",
    "    train_pos_reviews = load_pickled_data(train_pos_reviews_path)\n",
    "    train_reviews = train_neg_reviews + train_pos_reviews\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv = CountVectorizer()\n",
    "    X_train = cv.fit_transform(train_reviews)\n",
    "    pos_labels =[ 1 for i in range(12500)]\n",
    "    neg_labels = [0 for i in range(12500)]\n",
    "    Y_train= neg_labels + pos_labels\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train,Y_train)\n",
    "    test_neg_reviews = load_pickled_data(test_neg_reviews_path)\n",
    "    test_pos_reviews = load_pickled_data(test_pos_reviews_path)\n",
    "    test_reviews = test_neg_reviews + test_pos_reviews\n",
    "    X_test = cv.transform(test_reviews)\n",
    "    Y_test= neg_labels + pos_labels\n",
    "    pred_Y = clf.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"testing acc is\",accuracy_score(pred_Y,Y_test))\n",
    "    print(\"confusion matrix is\\n\",confusion_matrix(pred_Y,Y_test))\n",
    "    finish_time = datetime.datetime.now() - start_time\n",
    "    print(\"executed part 2 in\",finish_time ,\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "execute_part2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS535_21100313_A4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
